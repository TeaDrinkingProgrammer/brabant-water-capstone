{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from datetime import datetime, timedelta, date\n",
        "from pyspark.sql.types import TimestampType\n",
        "from pyspark.sql.functions import sum, to_date, col, when, month, dayofmonth, explode, array, struct, expr, lit , avg\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from shapely import wkt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Load dataframes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%run Meta/connection_information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#create mount point\n",
        "mssparkutils.fs.mount( \n",
        "    ADLS_PATH+\"/base/extern/wiwb/\", \n",
        "    \"/bwiwb/\", \n",
        "    {\"linkedService\":\"AzureDataLakeStorage1\"} \n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#load geojson polygon file\n",
        "cells = gpd.read_file(f\"/data/polygons_irc.geojson\")\n",
        "cells.index = cells[\"index\"]\n",
        "cells = cells.drop(columns=['index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#load dataframes\n",
        "BASE_GIS_FOLDER = '/base/wsbd/gis/WEB_Beheerregister_Afvalwaterketen/zuiveringseenheden'\n",
        "BASE_IRC_FOLDER = '/base/extern/wiwb/irc_combined'\n",
        "ENRICHED_FOLDER = '/enriched/extern/waterketen'\n",
        "\n",
        "# Needed to check when pipeline was last succesfull\n",
        "data_source = 'neerslag_per_zuivering'\n",
        "# Get today's date\n",
        "today_date = date.today()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def get_max_date_value(data_source: str) -> int:\n",
        "    try:\n",
        "        # Get the maximum timestamp value from the specified column\n",
        "        max_timestamp = spark.read.format('delta').load(f'{ENRICHED_FOLDER}/{data_source}').select(F.max(F.col(\"date_hour\"))).first()[0]\n",
        "        # Convert the maximum timestamp to a datetime object\n",
        "        max_datetime = datetime.strptime(str(max_timestamp), '%Y-%m-%d %H')\n",
        "        # Calculate the difference in days between max date and today\n",
        "        date_difference = (today_date - max_datetime.date()).days\n",
        "    except:\n",
        "        return 0\n",
        "    return date_difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# filter data based last date in the source file\n",
        "date_difference = get_max_date_value(data_source)\n",
        "days_ago = today_date - timedelta(days=date_difference)\n",
        "\n",
        "year, month, day = days_ago.year, days_ago.month, days_ago.day\n",
        "DATE_FROM = date(year, month, day)\n",
        "\n",
        "# load irc to DataFrame\n",
        "df_irc = spark.read \\\n",
        "            .format('delta') \\\n",
        "            .load(BASE_IRC_FOLDER)\n",
        "\n",
        "df_irc = df_irc.filter((df_irc.timestamp >= DATE_FROM))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Zet coordinaat systeem van de polygon geosjson om van rd new naar wgs84**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create a GeoDataFrame from the Pandas DataFrame\n",
        "cells = gpd.GeoDataFrame(cells, geometry=\"geometry\")\n",
        "\n",
        "# Define the RD New and WGS84 CRS\n",
        "crs_rd = \"EPSG:28992\"\n",
        "crs_wgs84 = \"EPSG:4326\"\n",
        "\n",
        "# Set the CRS of the original GeoDataFrame to RD New\n",
        "cells.crs = crs_rd\n",
        "\n",
        "# Convert the coordinates to WGS84 using .to_crs()\n",
        "cells_84 = cells.to_crs(crs_wgs84)\n",
        "\n",
        "# Plot the geometries using GeoPandas\n",
        "#cells.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Laad gislaag in**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# load zuiveringseenheden to DataFrame\n",
        "zuiveringseenheden_df = (\n",
        "    spark\n",
        "    .read\n",
        "    .format(\"delta\")\n",
        "    .load(BASE_GIS_FOLDER))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Convert the Spark DataFrame to a Pandas DataFrame\n",
        "zuiveringseenheden_df_pandas = zuiveringseenheden_df.toPandas()\n",
        "\n",
        "# Convert the WKT geometries to Shapely geometries\n",
        "zuiveringseenheden_df_pandas[\"geometry\"] = gpd.GeoSeries.from_wkt(zuiveringseenheden_df_pandas[\"geometrie\"])\n",
        "zuiveringseenheden_df_pandas = zuiveringseenheden_df_pandas.drop(columns=['geometrie'])\n",
        "\n",
        "# Create a GeoDataFrame from the Pandas DataFrame\n",
        "gpd_zuiveringseenheden = gpd.GeoDataFrame(zuiveringseenheden_df_pandas, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
        "\n",
        "# Plot the geometries using GeoPandas\n",
        "#zuiveringseenheden_df.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# # Create a new plot\n",
        "# fig, ax = plt.subplots()\n",
        "\n",
        "# # Plot the geometries from the first GeoDataFrame\n",
        "# cells_84.plot(ax=ax, color='blue', label='raster cellen WIWB')\n",
        "\n",
        "# # Plot the geometries from the second GeoDataFrame\n",
        "# gpd_zuiveringseenheden.plot(ax=ax, color='red', label='Zuiveringseenheden')\n",
        "\n",
        "# # Set plot title and legend\n",
        "# plt.title(\"Polygon laag ten opzichte van zuiveringseenhedn\")\n",
        "# plt.legend()\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Make an intersection of the zuiveringseenheden and the polygonen and use the selected polygons to select the right data from the wiwb dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Reset the index\n",
        "cells_84 = cells_84.reset_index()\n",
        "\n",
        "# Perform the spatial join using 'union' as the operation in overlay\n",
        "union_gdf = cells_84.overlay(gpd_zuiveringseenheden, how='union')\n",
        "\n",
        "union_gdf['area'] = union_gdf.area\n",
        "\n",
        "# Drop rows with NaN values in the specified column\n",
        "union_gdf = union_gdf.dropna(subset=['index','naam'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Calculate percentage of surface that belongs to the zuivering\n",
        "# Multiply the 'area' column by 10000 and round the percentage column\n",
        "union_gdf['percentage'] = (union_gdf['area'] * 10000 / 0.011895).round(0)\n",
        "\n",
        "# Drop geometry and area\n",
        "union_gdf = union_gdf.drop(columns=['geometry','area','naam'])\n",
        "\n",
        "# Convert Pandas DataFrame to PySpark DataFrame\n",
        "union_spark = spark.createDataFrame(union_gdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# resample the data to daily data, summing the precipitation\n",
        "df_irc = df_irc.groupBy(to_date('timestamp').alias('date'))\\\n",
        "                .sum()\n",
        "\n",
        "# adjust col names to match dict\n",
        "remove_sum_col_names = list(map(lambda x: x.replace(\"sum(\", \"\").replace(\")\", \"\"), df_irc.columns[1:]))\n",
        "df_irc = df_irc.toDF(\"date\", *remove_sum_col_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#function to melt dataframe\n",
        "def to_explode(df, by):\n",
        "\n",
        "    # Filter dtypes and split into column names and type description\n",
        "    cols, dtypes = zip(*((c, t) for (c, t) in df.dtypes if c not in by))\n",
        "    # Spark SQL supports only homogeneous columns\n",
        "    assert len(set(dtypes)) == 1, \"All columns have to be of the same type\"\n",
        "\n",
        "    # Create and explode an array of (column_name, column_value) structs\n",
        "    kvs = explode(array([\n",
        "      struct(lit(c).alias(\"polygon\"), col(c).alias(\"precipation\")) for c in cols\n",
        "    ])).alias(\"kvs\")\n",
        "\n",
        "    return df.select(by + [kvs]).select(by + [\"kvs.polygon\", \"kvs.precipation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "columns_to_drop = ['year', 'month', 'day']\n",
        "df_irc = df_irc.drop(*columns_to_drop)\n",
        "\n",
        "#start function\n",
        "df_irc_explode = to_explode(df_irc, ['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#join irc and percentage zuiveringseenheid\n",
        "df_irc_total = df_irc_explode.join(union_spark,df_irc_explode.polygon ==  union_spark.index,\"fullouter\")\n",
        "\n",
        "# Drop rows where \"index\" is null\n",
        "df_irc_total = df_irc_total.filter(col(\"index\").isNotNull())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#percentage surface * amount of precipation\n",
        "df_irc_total = df_irc_total\\\n",
        "  .withColumn('precipitation_percentage', (F.col('percentage')*F.col('precipation'))/100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#aggregate zuivering days and precipitation\n",
        "zuiveringen_precipitation = df_irc_total.groupby(\"date\",\"CODE\")\\\n",
        "    .agg(avg(\"precipitation_percentage\").cast('double').alias(\"precipitation\"),\\\n",
        "         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# save DataFrame\n",
        "zuiveringen_precipitation.write \\\n",
        "        .format(\"delta\") \\\n",
        "        .mode(\"append\") \\\n",
        "        .save(f'{ENRICHED_FOLDER}/neerslag_per_zuivering')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
